import json
import shutil
import warnings
import argparse

from lib.common import *
from lib.validation_functions import *

from entity_source_map import *
from template_from_table import generate_template

# Suppress the specific warning from openpyxl
warnings.simplefilter("ignore", UserWarning)

def validate_data(df, mandatory_columns,
                   string_columns_with_size, indexes, enum_names_with_values):
    # Copy the df to be able to progressively filter it while validating all the errors.
    result_df = df.copy().drop('PwCErrorReason', axis=1).drop('PwCWarnReason', axis=1)
    
    # For each mandatory field, make sure every row is populated.
    # result_df will be generated after dropping the rows with missing mandatory fields.
    result_df = validateMandatoryFields(df, result_df, mandatory_columns)

    # For each string field, make sure it does not exceed maximum length...
    # result_df will be generated by dropping any string which causes loss of data after truncation.
    result_df = validateStringFields(df, result_df, string_columns_with_size)

    # Validate all the unique indexes are maintained properly. Make sure to run this after truncating the strings.
    # result_df will be generated after dropping every row where duplicate values are found for given index. 
    result_df = validateIndexIntegrity(df, result_df, indexes)

    # Validate the values are either numbers indicating a valid position in the enum, or backend value.
    # result_df will be generated after dropping all rows with invalid enum position or backend value.
    #result_df = validateEnumFields(df, result_df, enum_names_with_values, excel_to_d365_mapping)

    return result_df, df

legal_entity_map = {
    'BCL' :	'BCHL',
    'DAT' :	'DATA',
    'TFNL':	'EHIL',
    'FMN' :	'FMNP',
    'GFZ' :	'GFCP',
    'GPPC':	'GPPL',
    'GSC' :	'GSCL',
    'GTC' :	'GTCD',
    'EFML':	'NEFM',
    'KNO' :	'NNFM',
    'ROM' :	'PEOP',
    'AEL' : 'GFCP',
    'AASL': 'GFCP',
    'GAI' : 'GFCP',
    'GSCN': 'ABTL',
    'OLTL': 'ABTL',
    'KFL' : 'PFMC',
    'AAFS': 'GFCP',
    'NGM' : 'FMNP'
}

def validateDataframe(input_df, template, indexes, companies=None):
    #To quickly identify which data column maps to which template column, convert all columns to upper.
    input_df.columns = input_df.columns.astype(str).str.upper()
    
    #Update DF index by 2, accounting for header & 0 based index
    input_df.set_index(pd.Index(range(2, len(input_df) + 2)), inplace=True)

    input_df.replace('NULL', '', inplace=True)
    #Drops fully empty columns(NULL or 0s included) from the data.
    input_df1 = input_df.copy()
    input_df1.replace('', pd.NA, inplace=True)
    input_df1.replace(0, pd.NA, inplace=True)
    empty_columns = input_df1.columns[input_df1.isna().all()]
    input_df = input_df.drop(empty_columns, axis=1)
    del input_df1

    # Adding a new column to store all the validation errors.
    input_df['PwCWarnReason'] = ''
    input_df['PwCErrorReason'] = ''
    #This will store the LE wise separated and validated data.
    separated_data, validated_data, error_data = {}, {}, {}

    # This checks the data types are proper or not and casts the columns appropriately...
    # ... additionally it also verifies all the mandatory columns are present.
    try:
        parsed_df, mandatory_columns, string_columns_with_size, enum_names_with_values = \
            parseDataWithTemplate(input_df, template)
    except Exception as e:
        raise e
    
    #If a column named DATAAREAID is present then it validates for each LE separately
    if 'DATAAREAID' in parsed_df.columns:
        parsed_df.loc[:, 'DATAAREAID'] = parsed_df['DATAAREAID'].astype(str)
        parsed_df.loc[:, 'DATAAREAID'] = parsed_df['DATAAREAID'].str.upper()
        parsed_df.loc[:, 'DATAAREAID'] = parsed_df.loc[:, 'DATAAREAID'].replace(legal_entity_map)

        grouped_df = parsed_df.groupby('DATAAREAID')
        for company, group_df in grouped_df:
            if company == '':
                input_df.loc[group_df.index, 'PwCErrorReason'] += 'Unspecified Legal Entity;'
                separated_data['Unspecified'] = group_df
                continue
            if companies is not None and company not in companies: continue
            separated_data[company] = group_df.drop('PwCErrorReason', axis=1).drop('PwCWarnReason', axis=1)
            print(f'Validating company: {company}')
            validated_df, group_df = validate_data(group_df, mandatory_columns,
                string_columns_with_size, indexes, enum_names_with_values)
            if validated_df.shape[0] > 0:
                validated_data[company] = validated_df
            if group_df[(group_df['PwCErrorReason'] != '')].shape[0] > 0:
                error_data[company] = group_df[(group_df['PwCErrorReason'] != '')]
    else:
        if 'COMPANY' in parsed_df.columns:
            parsed_df.loc[:, 'COMPANY'] = parsed_df['COMPANY'].astype(str)
            parsed_df.loc[:, 'COMPANY'] = parsed_df['COMPANY'].str.upper()
            parsed_df.loc[:, 'COMPANY'] = parsed_df.loc[:, 'COMPANY'].replace(legal_entity_map)
        validated_df, parsed_df = validate_data(parsed_df, mandatory_columns,
            string_columns_with_size, indexes, enum_names_with_values)
        if validated_df.shape[0] > 0:
                validated_data['GLOBAL'] = validated_df
        if parsed_df[(parsed_df['PwCErrorReason'] != '')].shape[0] > 0:
            error_data['GLOBAL'] = parsed_df[(parsed_df['PwCErrorReason'] != '')]
    
    return separated_data, validated_data, error_data,\
        input_df[(input_df['PwCWarnReason'] != '')]

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-d', '--datapath', help='Path to data directory')
    parser.add_argument('-t', '--tmplpath', help='Path to stored templates directory')
    parser.add_argument('-c', '--companies', nargs='+', help='Only validate for specific companies')
    args, names = parser.parse_known_args()
    
    if args.datapath is None and os.path.exists('data'): args.datapath = 'data'
    
    if args.tmplpath is None and os.path.exists('templates'): args.tmplpath = 'templates'
    if args.tmplpath is None: os.makedirs('templates/')
    
    # If no -c args provided, it defaults to the file res/companies.txt.
    # Empty -c argument will delete this file and remove any company specific validation.
    # Mentioning some companies in -c will reset the file res/companies.txt with the new companies.
    # Example: python data_validation -c fmnp bagl gscl hfmp
    if args.companies is None:
        try:
            with open('res/companies.txt', 'r') as file: args.companies = json.load(file)
            if len(args.companies) == 0:
                args.companies = None
                if os.path.exists('res/companies.txt'): os.remove('res/companies.txt')
        except FileNotFoundError: pass
    else:
        with open('res/companies.txt', 'w') as file: json.dump(args.companies, file)
    
    
    if args.companies is not None:
        print(f"Only validating the data for the companies: {args.companies}")

    for relative_path, file_name, file_extension in list_files_recursive(args.datapath):
        try: entityInfo = getEntityInfo(decode_filename(file_name))
        except ValueError: continue

        if args.tmplpath is None:
            template, indexes = generate_template(entity_info['Staging Table'].astype(str).iloc[0], force=False)
            pd.DataFrame(template).to_excel(f'templates/{file_name}.xlsx', index=False)
            with open(f'templates/indexes/{file_name}.txt', 'w') as file: json.dump(indexes, file)
        else:
            template = pd.read_excel(f'{args.tmplpath}/{file_name}.xlsx').to_dict(orient='records')
            with open(f'{args.tmplpath}/indexes/{file_name}.txt', 'r') as file: indexes = json.load(file)

        base_path = f'validation-results/{relative_path}/{file_name}'
        if os.path.exists(f'{base_path}'):
            shutil.rmtree(f'{base_path}')
        os.makedirs(f'{base_path}')
        
        if file_extension == '.csv':
            input_df = pd.read_csv(f'{args.datapath}/{file_name}{file_extension}', keep_default_na=False, encoding_errors='replace', low_memory=False)
        else:
            input_df = pd.read_excel(f'{args.datapath}/{file_name}{file_extension}', keep_default_na=False)
        
        def find_mixed_type_columns(df):
            mixed_type_columns = []
            for col in df.columns:
                if df[col].apply(type).nunique() > 1:
                    mixed_type_columns.append(col)
            return mixed_type_columns

        mixed_type_columns = find_mixed_type_columns(input_df)

        def convert_columns_to_str(df, columns):
            for col in columns:
                df[col] = df[col].astype(str)
            return df

        input_df = convert_columns_to_str(input_df, mixed_type_columns)

        with pd.ExcelWriter(f'{base_path}/{file_name}.xlsx') as writer:
            input_df.to_excel(writer, sheet_name='Raw Data', index=False)

        try:
            separated_data, validated_data, error_data, warnings_df = \
                validateDataframe(input_df, template, indexes, args.companies)
        except Exception as e: print(e)

        if len(separated_data.keys()) != 0:
            with pd.ExcelWriter(f'{base_path}/{file_name}_separated.xlsx') as writer:
                for company, company_df in separated_data.items():
                    company_df.to_excel(writer, sheet_name = company, index = False)
        
        if not warnings_df.empty:
            with pd.ExcelWriter(f'{base_path}/{file_name}_warnings.xlsx') as writer:
                warnings_df.to_excel(writer, sheet_name='Warnings', index=False)
                pd.DataFrame(template).to_excel(writer, sheet_name='Template', index=False)
        
        if len(error_data.keys()) != 0:
            with pd.ExcelWriter(f'{base_path}/{file_name}_errors.xlsx') as writer:
                for company, company_df in error_data.items():
                    company_df.to_excel(writer, sheet_name = company, index = False)

        if len(validated_data.keys()) != 0:
            print(file_name)
            with pd.ExcelWriter(f'{base_path}/{file_name}_validated.xlsx') as writer:
                for company, company_df in validated_data.items():
                    company_df.to_excel(writer, sheet_name = company, index = False)
