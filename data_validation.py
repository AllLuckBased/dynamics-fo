import warnings
import argparse
from bidict import ValueDuplicationError

from lib.common import *
from lib.validation_functions import *
from lib.dependency_validation import validate_dependencies

from entity_source_map import *
from template_from_table import generate_template

# Suppress the specific warning from openpyxl
warnings.simplefilter("ignore", UserWarning)

def validate_data(df, excel_to_d365_mapping, mandatory_columns, 
                  string_columns_with_size, indexes, enum_names_with_values, logs):
    # Copy the df to be able to progressively filter it while validating all the errors.
    result_df = df.copy()

    # For each mandatory field, make sure every row is populated.
    # result_df will be generated after dropping the rows with missing mandatory fields.
    result_df = validateMandatoryFields(df, result_df, mandatory_columns, excel_to_d365_mapping, logs)

    # For each string field, make sure it does not exceed maximum length...
    # result_df will be generated by dropping any string which causes loss of data after truncation.
    result_df = validateStringFields(df, result_df, string_columns_with_size, excel_to_d365_mapping, logs)

    # Validate all the unique indexes are maintained properly. Make sure to run this after truncating the strings.
    # result_df will be generated after dropping every row where duplicate values are found for given index. 
    result_df = validateIndexIntegrity(df, result_df, indexes, excel_to_d365_mapping, logs)

    # Validate the values are either numbers indicating a valid position in the enum, or backend value.
    # result_df will be generated after dropping all rows with invalid enum position or backend value.
    #result_df = validateEnumFields(df, result_df, enum_names_with_values, excel_to_d365_mapping, logs)

    return result_df

def validateDataframe(input_df, template, indexes, stagingRelations, entityRelations, all_entity_source_maps):
    #Update DF index by 2, accounting for header & 0 based index
    input_df.set_index(pd.Index(range(2, len(input_df) + 2)), inplace=True)
    input_df.replace('NULL', '', inplace=True)

    #Drops fully empty columns(NULL or 0s included) from the data. TODO: Verify this...
    input_df1 = input_df.copy()
    input_df1.replace('', pd.NA, inplace=True)
    input_df1.replace(0, pd.NA, inplace=True)
    empty_columns = input_df1.columns[input_df1.isna().all()]
    input_df = input_df.drop(empty_columns, axis=1)
    del input_df1

    # This will return details about errors generated while validation for this particular data.
    logs = {'': []}
    if input_df.shape[0] == 0:
        logs[''].append(NoValidDataError())
        return (input_df, {}, logs)

    #This will return the filtered data after validation
    validated_data = {}

    # This will parse the template and generate a mapping between data columns and template columns.
    # Additionally also casts the string columns into str if not already so for out input_df.
    # Note: While mandatory_column has template column names, both string and enum lists have data column names.
    try:
        excelToTemplateColumnMapping, mandatory_columns, string_columns_with_size, enum_names_with_values = \
            parseDataWithTemplate(input_df, template)
    # Exception is raised if data is missing a mandatory column.
    except ValueError as e:
        logs[''].append(MissingColumnError(e))
        return (input_df, {}, logs)

    # Search the data for a column with a name like dataareaid.
    # If found, group validate the df based on it, if not then validate the entirety of the df at once.
    entity_dependencies = {}
    for column in input_df.columns:
        if column.lower() == 'dataareaid':
            grouped_df = input_df.groupby(column)
            for company, group_df in grouped_df:
                logs[company] = []
                if company == '':
                    logs[''].append(MissingDataError(column, ', '.join(map(str, group_df.index))))
                    continue
                entity_dependencies.update(validate_dependencies(group_df, company, excelToTemplateColumnMapping,
                    stagingRelations, entityRelations, all_entity_source_maps, logs[company]))
                validated_data[company] = validate_data(
                    group_df, excelToTemplateColumnMapping, mandatory_columns,
                    string_columns_with_size, indexes, enum_names_with_values, logs[company])
                if validated_data[company].shape[0] == 0:
                    logs[company].append(NoValidDataError())
            break
    else:
        entity_dependencies.update(validate_dependencies(input_df, None,
                    excelToTemplateColumnMapping, stagingRelations, entityRelations, all_entity_source_maps, logs['']))
        validated_data[''] = validate_data(input_df, excelToTemplateColumnMapping, mandatory_columns, 
            string_columns_with_size, indexes, enum_names_with_values, logs[''])
    
    if len(logs.keys()) > 1 and '' in logs: logs.pop('')
    return (validated_data, entity_dependencies, logs)

#TODO: Make tree
class DependencyTreeNode:
    def __init__(self, name, dependencies):
        self.name = name
        self.dependencies = dependencies

    def get_execution_level(self):
        if len(self.dependencies) == 0:
            return 1
        else:
            max = 1
            for dependency in self.dependencies:
                if dependency.get_execution_level() > max:
                    max = dependency.execution_level
            return max

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-d', '--datapath', help='Path to data directory')
    parser.add_argument('-c', '--clean', action='store_true', help='Clears all cached data from previous runs.')
    args, names = parser.parse_known_args()
    
    os.makedirs('cache', exist_ok=True)
    #args.datapath = 'data'
    if args.clean:
        shutil.rmtree('cache')
    elif args.datapath:
        #shutil.rmtree('output/')
        #shutil.rmtree('cache/data')
        correctFolder(args.datapath, 'cache/data')
    else:
        #TODO: adds the new file to the cache from the data. Maybe it does it inside args.datapath only instead of doing current behaviour...
        args.datapath = 'cache/data'
        if not os.path.exists(args.datapath):
            print('Could not find any data to validate. Please specify -d command line argument with data directory path.')
            exit(-1)

    all_entity_source_maps = bidict()
    if not os.path.exists('cache/all_entity_source_maps.xlsx'):
        scoped_entities = pd.read_excel('cache/scope.xlsx', 
            header=None, names=['Data Entity'])['Data Entity'].tolist()
        
        rows = []
        for entity in scoped_entities:
            entity_info = getEntityInfo(entity)
            table_sources = get_all_data_sources(
                entity_info['Target Entity'].astype(str).iloc[0], get_dependencies=False)
            for entity_field, source_map in table_sources.items():
                if source_map[0] != '' and source_map[1] != '':
                    try:
                        all_entity_source_maps[((entity), (entity_field))] = tuple(source_map)
                        rows.append({
                            'Entity Name': entity,
                            'Entity Field': entity_field,
                            'Source Table': source_map[0],
                            'Source Field': source_map[1],
                        })
                    except ValueDuplicationError:
                        prevKey = all_entity_source_maps.inv[tuple(source_map)]
                        all_entity_source_maps.pop(prevKey)
                        all_entity_source_maps[(prevKey[0] + (entity), (prevKey[1]) + (entity_field))]\
                            = tuple(source_map)
        pd.DataFrame(rows).to_excel('cache/all_entity_source_maps.xlsx', index=False)
    else:
        #TODO: cache relations
        df = pd.read_excel('cache/all_entity_source_maps.xlsx')
        for _, row in df.iterrows():
            entity = row['Entity Name']
            entity_field = row['Entity Field']
            source_table = row['Source Table']
            source_field = row['Source Field']
            
            try:
                all_entity_source_maps[((entity,), (entity_field,))] = (source_table, source_field)
            except ValueDuplicationError:
                prevKey = all_entity_source_maps.inv[(source_table, source_field)]
                all_entity_source_maps.pop(prevKey)
                all_entity_source_maps[(prevKey[0] + (entity,), (prevKey[1]) + (entity_field,))]\
                    = (source_table, source_field)


    rows = []
    dependency_rows = []
    for relative_path, file_name, file_extension in list_files_recursive(args.datapath):
        entity_info = getEntityInfo(file_name)
        
        #TODO: Cache this too...
        template, indexes, stagingRelations = \
            generate_template(entity_info['Staging Table'].astype(str).iloc[0], force=False)
        entityRelations = get_entity_relations(entity_info['Target Entity'].astype(str).iloc[0])

        if file_extension == '.csv':
            input_df = pd.read_csv(f'{args.datapath}/{file_name}{file_extension}',
                keep_default_na=False, low_memory=False)
        elif file_extension == '.xlsx':
            input_df = pd.read_excel(f'{args.datapath}/{file_name}{file_extension}',
                keep_default_na=False, low_memory=False)
        else:
            continue
    
        validated_data, entity_dependencies, logs = \
            validateDataframe(input_df, template, indexes, stagingRelations, entityRelations, all_entity_source_maps)

        base_path = f'output/{relative_path}/{file_name}'
        if not os.path.exists(f'{base_path}/logs'):
            os.makedirs(f'{base_path}/logs')
        
        with pd.ExcelWriter(f'{base_path}/{file_name}_validated.xlsx') as writer:
            for company, company_df in validated_data.items():
                company_df.to_excel(writer, sheet_name = company if company != '' else file_name, index = False)
        
        for column, dependency in entity_dependencies.items():
            dependency_rows.append({
                "Entity Name": file_name,
                "Entity Column": column,
                "Related Entity": dependency[0][0],
                "Related Column": dependency[1][0],
                "Fixed Constraint": dependency[3],
                "Optional": dependency[2],
            })

        error_types = {}
        valid_companies = []
        for company, errors in logs.items():
            row = {
                'Entity Name': entity_info['Data Entity'].astype(str).iloc[0],
                'Company': company,
                'Severity': 0
            }

            if len(errors) == 0:
                valid_companies.append(company)
            for error in errors:
                if error.severity > row['Severity']:
                    row['Severity'] = error.severity
                row[type(error).__name__] = error.shortLog()
                
                with open(f'{base_path}/logs/{company}.txt', 'a') as file:
                    file.write(f'{error.log()}\n')
                
                if error.error in error_types:
                    error_types[error.error].add(company)
                else:
                    error_types[error.error] = set([company])
            rows.append(row)
        with open(f'{base_path}/overall_logs.txt', 'a') as file:
            for error, companies in error_types.items():
                file.write(f'{error}\nFor DATAAREAID(s): {companies}\n\n')

            file.write(f'Valid DATAAREAID(s) were:\n {valid_companies}\n\n')

    pd.DataFrame(rows).to_excel(f'output/overall_report.xlsx', index=False)
    pd.DataFrame(dependency_rows).to_excel(f'output/dependencies.xlsx', index=False)
