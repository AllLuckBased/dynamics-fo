import os
import argparse
import pandas as pd
from collections import defaultdict

from lib import *
from validation_functions import *
from template_from_table import generate_template

def validate_data(df, excel_to_d365_mapping, mandatory_columns, string_columns_with_size, 
                  indexes, enum_names_with_values, logs):
    # Copy the df to be able to progressively filter it while validating all the errors.
    result_df = df.copy()

    # For each mandatory field, make sure every row is populated.
    # result_df will be generated after dropping the rows with missing mandatory fields.
    result_df = validateMandatoryFields(df, result_df, mandatory_columns, excel_to_d365_mapping, logs)

    # For each string field, make sure it does not exceed maximum length...
    # result_df will be generated by dropping any string which causes loss of data after truncation.
    result_df = validateStringFields(df, result_df, string_columns_with_size, excel_to_d365_mapping, logs)

    # Validate all the unique indexes are maintained properly. Make sure to run this after truncating the strings.
    # result_df will be generated after dropping every row where duplicate values are found for given index. 
    result_df = validateIndexIntegrity(df, result_df, indexes, excel_to_d365_mapping, logs)

    # Validate the values are either numbers indicating a valid position in the enum, or backend value.
    # result_df will be generated after dropping all rows with invalid enum position or backend value.
    result_df = validateEnumFields(df, result_df, enum_names_with_values, excel_to_d365_mapping, logs)

    return result_df

def validateDataframe(input_df, entity_info, companies_to_consider):
    #Update DF index by 2, accounting for header & 0 based index
    input_df.set_index(pd.Index(range(2, len(input_df) + 2)), inplace=True)
    
    #Drops fully empty columns(NULL or 0s included) from the data.
    input_df1 = input_df.copy()
    input_df1.replace('', pd.NA, inplace=True)
    input_df1.replace(0, pd.NA, inplace=True)
    input_df1.replace('NULL', pd.NA, inplace=True)
    empty_columns = input_df1.columns[input_df1.isna().all()]
    input_df = input_df.drop(empty_columns, axis=1)
    del input_df1

    #This will return the filtered data after validation
    validated_data = defaultdict(pd.DataFrame)
    # This will return details about errors generated while validation for this particular data.
    logs = defaultdict(list)
    
    #Generate the template of the given entity
    template, indexes = generate_template(entity_info['Staging Table'].astype(str).iloc[0], force=False)

    # This will parse the template and generate a mapping between data columns and template columns.
    # Additionally also casts the string columns into str if not already so for out input_df.
    # Note: While mandatory_column has template column names, both string and enum lists have data column names.
    try:
        excelToTemplateColumnMapping, mandatory_columns, string_columns_with_size, enum_names_with_values = \
            parseDataWithTemplate(input_df, template)
    # Exception is raised if data is missing a mandatory column.
    except ValueError as e:
        logs[''].append(MissingColumnError(e))

    # Search the data for a column with a name like dataareaid.
    # If found, group validate the df based on it, if not then validate the entirety of the df at once.
    for column in input_df.columns:
        if column.lower() == 'dataareaid':
            grouped_df = input_df.groupby(column)
            for company, group_df in grouped_df:
                if company == '':
                    logs[''].append(MissingDataError(column, ', '.join(map(str, grouped_df.index))))
                    continue
                if companies_to_consider and company.lower() not in companies_to_consider:
                    continue
                validated_data[company] = validate_data(
                    group_df, excelToTemplateColumnMapping, mandatory_columns, 
                    string_columns_with_size, indexes, enum_names_with_values, logs[company])
            break
    else:
        validated_data[''] = validate_data(input_df, excelToTemplateColumnMapping, mandatory_columns, 
            string_columns_with_size, indexes, enum_names_with_values, logs[''])
    return (validated_data, logs)

if __name__ == '__main__':
    #Parse the required arguments from command line.
    parser = argparse.ArgumentParser()
    parser.add_argument('-d', '--datapath', help='Path to data directory')
    parser.add_argument('-c', '--companies', nargs='+', help="Provide list of dataareaids to validate. Leave empty to validate all.")
    args, names = parser.parse_known_args()
    
    args.datapath = 'data'
    names = list_files_recursive(args.datapath)
    for (relative_path, file_name, file_extension) in names:
        try:
            entityInfo = getEntityInfo(file_name)
        except ValueError:
            continue
        
        if file_extension == '.csv':
            input_df = pd.read_csv(f'{args.datapath}/{file_name}{file_extension}',
                keep_default_na=False, low_memory=False)
        elif file_extension == '.xlsx':
            input_df = pd.read_excel(f'{args.datapath}/{file_name}{file_extension}',
                keep_default_na=False, low_memory=False)
        else:
            continue

        validated_data, logs = validateDataframe(input_df, entityInfo, args.companies)

        base_path = f'output/{relative_path}/{file_name}'
        if not os.path.exists(f'{base_path}/logs'):
            os.makedirs(f'{base_path}/logs')
        
        with pd.ExcelWriter(f'{base_path}/{file_name}_validated.xlsx') as writer:
            for company, company_df in validated_data.items():
                company_df.to_excel(writer, sheet_name = company if company != '' else file_name, index = False)

        for company, errors in logs.items():
            for error in errors:
                log(error.log(), f'{base_path}/logs/{company}.txt')
